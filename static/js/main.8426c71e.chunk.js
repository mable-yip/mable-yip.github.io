(this.webpackJsonpbase=this.webpackJsonpbase||[]).push([[0],{14:function(e,t){var a={ALL_PUBLICATION:"All Publication",BY_CATEGORY:"By Category"},i={TITLE:"Title",AUTHOR:"Author",YEAR:"Year"},n={layout:a.ALL_PUBLICATION,sortBy:i.TITLE};e.exports={pageSize:10,categoryPageSize:5,categoryType:{Journal:"Journal",Conference:"Conference",Book:"Book",Other:"Other"},layoutOptions:a,sortingOptions:i,defaultOption:n}},59:function(e,t,a){},60:function(e,t,a){"use strict";a.r(t);var i,n,o,s,r,c=a(0),l=a(21),d=a.n(l),u=a(19),p=(a(43),a(44),a(45),a(46),a(7)),h=a(65),g=a(31),m=a(63),f=Object({NODE_ENV:"production",PUBLIC_URL:"",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0,REACT_APP_TEAM_HOMEPAGE:'{"teamId":"612b849beae47c89542bebca","aboutUs":[""]}',REACT_APP_TEAM_INFO:'{"_id":"612b849beae47c89542bebca","teamName":"test","orgName":"test","email":"pmyip@gmail.com"}',REACT_APP_TEAM_MEMBERS:'[{"_id":"612dc016de93cd7af48cc167","fullName":"rewgwe","position":"rgewrg","summary":"wergewrgwer"}]',REACT_APP_TEAM_PUBLICATIONS:'[{"_id":"612b9a8e52fad35524a80352","authors":["Xiao Chen","Wanli Chen","Kui Liu","Chunyang Chen","Li Li"],"title":"A comparative study of smartphone and smartwatch apps","link":"","description":"Despite that our community has spent numerous efforts on analyzing mobile apps, there is no study proposed for characterizing the relationship between smartphone and smartwatch apps. To fill this gap, we present to the community a comparative study of smartphone and smartwatch apps, aiming at understanding the status quo of cross-phone/watch apps. Specifically, in this work, we first collect a set of cross-phone/watch app pairs and then experimentally look into them to explore their similarities or dissimilarities from different perspectives. Experimental results show that (1) Approximately, up to 40% of resource files, 30% of code methods are reused between smartphone/watch app pairs,(2) Smartphone apps may require more than twice as many as permissions and adopt more than five times as many as user interactions than their watch counterparts, and (3) Smartwatch apps can be released as either\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"Book","categoryTitle":"Proceedings of the 36th Annual ACM Symposium on Applied Computing","pages":"1484-1493","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.039Z","updatedAt":"2021-08-29T14:32:46.039Z","year":2021},{"_id":"612b9a8e52fad35524a80351","authors":["Sidong Feng","Suyu Ma","Jinzhong Yu","Chunyang Chen","TingTing Zhou","Yankun Zhen"],"title":"Auto-icon: An automated code generation tool for icon designs assisting in ui development","link":"","description":"Approximately 50% of development resources are devoted to UI development tasks [8]. Occupied a large proportion of development resources, developing icons can be a time-consuming task, because developers need to consider not only effective implementation methods but also easy-to-understand descriptions. In this study, we define 100 icon classes through an iterative open coding for the existing icon design sharing website. Based on a deep learning model and computer vision methods, we propose an approach to automatically convert icon images to fonts with descriptive labels, thereby reducing the laborious manual effort for developers and facilitating UI development. We quantitatively evaluate the quality of our method in the real world UI development environment and demonstrate that our method offers developers accurate, efficient, readable, and usable code for icon images, in terms of saving 65.2\xa0\u2026","yearPublished":"2021","citedBy":1,"category":{"type":"Book","categoryTitle":"26th International Conference on Intelligent User Interfaces","pages":"59-69","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.039Z","updatedAt":"2021-08-29T14:32:46.039Z","year":2021},{"_id":"612b8ad0de6d5573680cef1c","authors":["Kaibo Cao","Chunyang Chen","Sebastian Baltes","Christoph Treude","Xiang Chen"],"title":"Automated Query Reformulation for Efficient Search based on Query Logs From Stack Overflow","link":"https://arxiv.org/pdf/2102.00826","description":"As a popular Q&A site for programming, Stack Overflow is a treasure for developers. However, the amount of questions and answers on Stack Overflow make it difficult for developers to efficiently locate the information they are looking for. There are two gaps leading to poor search results: the gap between the user\u2019s intention and the textual query, and the semantic gap between the query and the post content. Therefore, developers have to constantly reformulate their queries by correcting misspelled words, adding limitations to certain programming languages or platforms, etc. As query reformulation is tedious for developers, especially for novices, we propose an automated software-specific query reformulation approach based on deep learning. With query logs provided by Stack Overflow, we construct a large-scale query reformulation corpus, including the original queries and corresponding reformulated ones\xa0\u2026","yearPublished":"2021","citedBy":5,"category":{"type":"Other","categoryTitle":"","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T13:25:36.346Z","updatedAt":"2021-08-29T13:25:36.346Z","year":2021},{"_id":"612b8ad0de6d5573680cef17","authors":["Junjie Wang","Ye Yang","Song Wang","Chunyang Chen","Dandan Wang","Qing Wang"],"title":"Context-aware Personalized Crowdtesting Task Recommendation","link":"","description":"Crowdsourced software testing (short for crowdtesting) is a special type of crowdsourcing. It requires that crowdworkers master appropriate skill-sets and commit significant effort for completing a task. Abundant uncertainty may arise during a crowdtesting process due to imperfect information between the task requester and crowdworkers. For example, a worker frequently chooses tasks in an ad hoc manner in crowdtesting context, and an inappropriate task selection may lead to the worker\'s failing to detect any bugs, and significant testing effort unpaid and wasted. Recent studies have explored methods for supporting task requesters to make informed decisions on task pricing, worker recommendation, and so on. Unfortunately, very few study offers decision making support from the crowdworkers\' perspectives. We motivate this study through a pilot study, revealing the large portion (74\\\\%) of unpaid crowdworkers\'\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"Other","categoryTitle":"","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T13:25:36.346Z","updatedAt":"2021-08-29T13:25:36.346Z","year":2021},{"_id":"612b9a8e52fad35524a80353","authors":["Yuanchun Li","Jiayi Hua","Haoyu Wang","Chunyang Chen","Yunxin Liu"],"title":"DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection","link":"https://arxiv.org/pdf/2101.06896","description":"Deep learning models are increasingly used in mobile applications as critical components. Unlike the program bytecode whose vulnerabilities and threats have been widely-discussed, whether and how the deep learning models deployed in the applications can be compromised are not well-understood since neural networks are usually viewed as a black box. In this paper, we introduce a highly practical backdoor attack achieved with a set of reverse-engineering techniques over compiled deep learning models. The core of the attack is a neural conditional branch constructed with a trigger detector and several operators and injected into the victim model as a malicious payload. The attack is effective as the conditional logic can be flexibly customized by the attacker, and scalable as it does not require any prior knowledge from the original model. We evaluated the attack effectiveness using 5 state-of-the-art deep\xa0\u2026","yearPublished":"2021","citedBy":2,"category":{"type":"Conference","categoryTitle":"43rd International Conference on Software Engineering (ICSE)","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.039Z","updatedAt":"2021-08-29T14:32:46.039Z","year":2021},{"_id":"612b8ad0de6d5573680cef1b","authors":["Han Wang","Chunyang Chen","Zhenchang Xing","John Grundy"],"title":"DiffTech: Differencing Similar Technologies from Crowd-Scale Comparison Discussions","link":"","description":"Developers use different technologies for many software development tasks. However, when faced with several technologies with comparable functionalities, it is not easy to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers can resort to expert articles, read official documents or ask questions in Q&A sites. However, it still remains difficult to get a comprehensive comparison as online information is often fragmented or contradictory. To overcome these limitations, we propose the DiffTech system that exploits crowdsourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different aspects. We first build a large database of comparable technologies in software engineering by mining tags in Stack Overflow. We then locate comparative sentences about comparable technologies with\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"Other","categoryTitle":"","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T13:25:36.346Z","updatedAt":"2021-08-29T13:25:36.346Z","year":2021},{"_id":"612b8ad0de6d5573680cef16","authors":["Bo Yang","Zhenchang Xing","Xin Xia","Chunyang Chen","Deheng Ye","Shanping Li"],"title":"Don\u2019t Do That! Hunting Down Visual Design Smells in Complex UIs against Design Guidelines","link":"","description":"Just like code smells in source code, UI design has visual design smells. We study 93 don\u2019t-do-that guidelines in the Material Design, a complex design system created by Google. We find that these don\u2019t-guidelines go far beyond UI aesthetics, and involve seven general design dimensions (layout, typography, iconography, navigation, communication, color, and shape) and four component design aspects (anatomy, placement, behavior, and usage). Violating these guidelines results in visual design smells in UIs (or UI design smells). In a study of 60,756 UIs of 9,286 Android apps, we find that 7,497 UIs of 2,587 apps have at least one violation of some Material Design guidelines. This reveals the lack of developer training and tool support to avoid UI design smells. To fill this gap, we design an automated UI design smell detector (UIS-Hunter) that extracts and validates multi-modal UI information (component\xa0\u2026","yearPublished":"2021","citedBy":1,"category":{"type":"Other","categoryTitle":"","pages":"761-772","publisher":"IEEE","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T13:25:36.345Z","updatedAt":"2021-08-29T13:25:36.345Z","year":2021},{"_id":"612b8ad0de6d5573680cef1d","authors":["Tianming Zhao","Chunyang Chen","Yuanning Liu","Xiaodong Zhu"],"title":"GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Networks","link":"https://arxiv.org/pdf/2101.09978","description":"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications, and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. Besides, the requirement of the rapid development of GUI design also aggravates designers\u2019 working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model GUIGAN to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our GUIGAN is to reuse GUI components collected from existing mobile app GUIs for composing a new design that is similar to natural-language generation. Our\xa0\u2026","yearPublished":"2021","citedBy":2,"category":{"type":"Other","categoryTitle":"","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T13:25:36.346Z","updatedAt":"2021-08-29T13:25:36.346Z","year":2021},{"_id":"612b8ad0de6d5573680cef18","authors":["Qiuyuan Chen","Chunyang Chen","Safwat Hassan","Zhengchang Xing","Xin Xia","Ahmed E Hassan"],"title":"How Should I Improve the UI of My App? A Study of User Reviews of Popular Apps in the Google Play","link":"","description":"UI (User Interface) is an essential factor influencing users\u2019 perception of an app. However, it is hard for even professional designers to determine if the UI is good or not for end-users. Users\u2019 feedback (e.g., user reviews in the Google Play) provides a way for app owners to understand how the users perceive the UI. In this article, we conduct an in-depth empirical study to analyze the UI issues of mobile apps. In particular, we analyze more than 3M UI-related reviews from 22,199 top free-to-download apps and 9,380 top non-free apps in the Google Play Store. By comparing the rating of UI-related reviews and other reviews of an app, we observe that UI-related reviews have lower ratings than other reviews. By manually analyzing a random sample of 1,447 UI-related reviews with a 95% confidence level and a 5% interval, we identify 17 UI-related issues types that belong to four categories (i.e., \u201cAppearance\xa0\u2026","yearPublished":"2021","citedBy":3,"category":{"type":"Other","categoryTitle":"","pages":"1-38","publisher":"ACM","volume":"30","issue":"3"},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T13:25:36.346Z","updatedAt":"2021-08-29T13:25:36.346Z","year":2021},{"_id":"612b8ad0de6d5573680cef14","authors":["Yuhui Su","Zhe Liu","Chunyang Chen","Junjie Wang","Qing Wang"],"title":"OwlEyes-online: a fully automated platform for detecting and localizing UI display issues","link":"https://arxiv.org/pdf/2107.02364","description":"Graphical User Interface (GUI) provides visual bridges between software apps and end users. However, due to the compatibility of software or hardware, UI display issues such as text overlap, blurred screen, image missing always occur during GUI rendering on different devices. Because these UI display issues can be found directly by human eyes, in this paper, we implement an online UI display issue detection tool OwlEyes-Online, which provides a simple and easy-to-use platform for users to realize the automatic detection and localization of UI display issues. The OwlEyes-Online can automatically run the app and get its screenshots and XML files, and then detect the existence of issues by analyzing the screenshots. In addition, OwlEyes-Online can also find the detailed area of the issue in the given screenshots to further remind developers. Finally, OwlEyes-Online will automatically generate test reports with UI display issues detected in app screenshots and send them to users. The OwlEyes-Online was evaluated and proved to be able to accurately detect UI display issues. Tool Link: http://www.owleyes.online:7476 Github Link: https://github.com/franklinbill/owleyes Demo Video Link: https://youtu.be/002nHZBxtCY","yearPublished":"2021","citedBy":null,"category":{"type":"Other","categoryTitle":"","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T13:25:36.345Z","updatedAt":"2021-08-29T13:25:36.345Z","year":2021},{"_id":"612b9a8e52fad35524a80354","authors":["Yujin Huang","Han Hu","Chunyang Chen"],"title":"Robustness of on-device Models: Adversarial Attack to Deep Learning Models on Android Apps","link":"https://arxiv.org/pdf/2101.04401","description":"Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models\u2019 exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the\xa0\u2026","yearPublished":"2021","citedBy":6,"category":{"type":"Conference","categoryTitle":"43rd International Conference on Software Engineering","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.039Z","updatedAt":"2021-08-29T14:32:46.039Z","year":2021},{"_id":"612b8ad0de6d5573680cef15","authors":["Bo Yang","Zhenchang Xing","Xin Xia","Chunyang Chen","Deheng Ye","Shanping Li"],"title":"UIS-Hunter: Detecting UI Design Smells in Android Apps","link":"","description":"Similar to code smells in source code, UI design has visual design smells that indicate violations of good UI design guidelines. UI design guidelines constitute design systems for a vast variety of products, platforms, and services. Following a design system, developers can avoid common design issues and pitfalls. However, a design system is often complex, involving various design dimensions and numerous UI components. Lack of concerns on GUI visual effect results in little support for detecting UI design smells that violate the design guidelines in a complex design system. In this paper, we propose an automated UI design smell detector named UIS-Hunter (UI design Smell Hunter). The tool is able to (i) automatically process UI screenshots or prototype files to detect UI design smells and generate reports, (ii) highlight the violated UI regions and list the material design guidelines that the found design smells\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"Other","categoryTitle":"","pages":"89-92","publisher":"IEEE","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T13:25:36.345Z","updatedAt":"2021-08-29T13:25:36.345Z","year":2021},{"_id":"612b9a8e52fad35524a80355","authors":["Han Wang","Chunyang Chen","Zhenchang Xing","John Grundy"],"title":"DiffTech: A tool for differencing similar technologies from question-and-answer discussions","link":"","description":"Developers can use different technologies for different software development tasks in their work. However, when faced with several technologies with comparable functionalities, it can be challenging for developers to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers resort to expert articles, read official documents or ask questions in Q&A sites for technology comparison. However, it is still very opportunistic whether they will get a comprehensive comparison, as online information is often fragmented, contradictory and biased. To overcome these limitations, we propose the DiffTech system that exploits the crowd sourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We found 19,118 comparative sentences from 2,410 pairs of comparable technologies\xa0\u2026","yearPublished":"2020","citedBy":1,"category":{"type":"Book","categoryTitle":"Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"1576-1580","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.039Z","updatedAt":"2021-08-29T14:32:46.039Z","year":2020},{"_id":"612b9a8e52fad35524a80358","authors":["Chunyang Chen","Sidong Feng","Zhengyang Liu","Zhenchang Xing","Shengdong Zhao"],"title":"From Lost to Found: Discover Missing UI Design Semantics through Recovering Missing Tags","link":"https://dl.acm.org/doi/pdf/10.1145/3415194","description":"Design sharing sites provide UI designers with a platform to share their works and also an opportunity to get inspiration from others\' designs. To facilitate management and search of millions of UI design images, many design sharing sites adopt collaborative tagging systems by distributing the work of categorization to the community. However, designers often do not know how to properly tag one design image with compact textual description, resulting in unclear, incomplete, and inconsistent tags for uploaded examples which impede retrieval, according to our empirical study and interview with four professional designers. Based on a deep neural network, we introduce a novel approach for encoding both the visual and textual information to recover the missing tags for existing UI examples so that they can be more easily found by text queries. We achieve 82.72% accuracy in the tag prediction. Through a simulation\xa0\u2026","yearPublished":"2020","citedBy":11,"category":{"type":"Journal","categoryTitle":"Proceedings of the ACM on Human-Computer Interaction","pages":"123:1-123:22","publisher":"ACM","volume":"4","issue":"CSCW2"},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.040Z","updatedAt":"2021-08-29T14:32:46.040Z","year":2020},{"_id":"612b9a8e52fad35524a80359","authors":["Jieshan Chen","Mulong Xie","Zhenchang Xing","Chunyang Chen","Xiwei Xu","Liming Zhu"],"title":"Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination?","link":"https://arxiv.org/pdf/2008.05132","description":"Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task. It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation. Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (eg, canny edge, contours), and deep learning models that learn to detect from large-scale GUI data. Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task. We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these\xa0\u2026","yearPublished":"2020","citedBy":21,"category":{"type":"Conference","categoryTitle":"(ESEC/FSE) ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.040Z","updatedAt":"2021-08-29T14:32:46.040Z","year":2020},{"_id":"612b9a8e52fad35524a80357","authors":["Zhe Liu","Chunyang Chen","Junjie Wang","Yuekai Huang","Jun Hu","Qing Wang"],"title":"Owl Eyes: Spotting UI Display Issues via Visual Understanding","link":"https://arxiv.org/pdf/2009.01417","description":"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the development of technology and aesthetics, the visual effects of the GUI are more and more attracting. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding\xa0\u2026","yearPublished":"2020","citedBy":8,"category":{"type":"Conference","categoryTitle":"2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.039Z","updatedAt":"2021-08-29T14:32:46.039Z","year":2020},{"_id":"612b9a8e52fad35524a8035c","authors":["Dehai Zhao","Zhenchang Xing","Chunyang Chen","Xiwei Xu","Liming Zhu","Guoqiang Li","Jinshui Wang"],"title":"Seenomaly: vision-based linting of GUI animation effects against design-don\'t guidelines","link":"","description":"GUI animations, such as card movement, menu slide in/out, snack-bar display, provide appealing user experience and enhance the usability of mobile applications. These GUI animations should not violate the platform\'s UI design guidelines (referred to as design-don\'t guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes. However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can \u201csee\u201d the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don\'t guidelines. In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier. Instead, we propose an unsupervised, computer-vision based adversarial\xa0\u2026","yearPublished":"2020","citedBy":15,"category":{"type":"Conference","categoryTitle":"42nd International Conference on Software Engineering (ICSE\u201920). ACM, New York, NY","pages":"1286 - 1297","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.040Z","updatedAt":"2021-08-29T14:32:46.040Z","year":2020},{"_id":"612cc3423c0bce8c741d7d7f","authors":["Chunyang Chen"],"title":"SimilarAPI: Mining Analogical APIs for Library Migration","link":"","description":"Establishing API mappings between libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined, and existing methods based on supervised learning requires unavailable already-ported or functionality similar applications. Therefore, we propose an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. We implement a proof-of-concept website SimilarAPI (https://similarapi.appspot.com) which can recommend analogical APIs for 583,501 APIs of 111 pairs of analogical Java libraries with diverse functionalities. Video: https://youtu.be/EAwD6l24vLQ","yearPublished":"2020","citedBy":3,"category":{"type":"Conference","categoryTitle":"International Conference on Software Engineering (ICSE), Demo Track","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-30T11:38:42.229Z","updatedAt":"2021-08-30T11:38:42.229Z","year":2020},{"_id":"612b9a8e52fad35524a80356","authors":["Mulong Xie","Sidong Feng","Zhenchang Xing","Jieshan Chen","Chunyang Chen"],"title":"UIED: a hybrid tool for GUI element detection","link":"https://www.researchgate.net/profile/Mulong-Xie/publication/346170544_UIED_a_hybrid_tool_for_GUI_element_detection/links/5fbca13a458515b797641b72/UIED-a-hybrid-tool-for-GUI-element-detection.pdf","description":"Graphical User Interface (GUI) elements detection is critical for many GUI automation and GUI testing tasks. Acquiring the accurate positions and classes of GUI elements is also the very first step to conduct GUI reverse engineering or perform GUI testing. In this paper, we implement a User Iterface Element Detection (UIED), a toolkit designed to provide user with a simple and easy-to-use platform to achieve accurate GUI element detection. UIED integrates multiple detection methods including old-fashioned computer vision (CV) approaches and deep learning models to handle diverse and complicated GUI images. Besides, it equips with a novel customized GUI element detection methods to produce state-of-the-art detection results. Our tool enables the user to change and edit the detection result in an interactive dashboard. Finally, it exports the detected UI elements in the GUI image to design files that can be\xa0\u2026","yearPublished":"2020","citedBy":5,"category":{"type":"Book","categoryTitle":"Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"1655-1659","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.039Z","updatedAt":"2021-08-29T14:32:46.039Z","year":2020},{"_id":"612b9a8e52fad35524a8035b","authors":["Jieshan Chen","Chunyang Chen","Zhenchang Xing","Xiwei Xu","Liming Zhu","Guoqiang Li","Jinshui Wang"],"title":"Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning","link":"https://arxiv.org/pdf/2003.00380","description":"According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users\' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android\xa0\u2026","yearPublished":"2020","citedBy":29,"category":{"type":"Conference","categoryTitle":"42nd International Conference on Software Engineering (ICSE\u201920)","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.040Z","updatedAt":"2021-08-29T14:32:46.040Z","year":2020},{"_id":"612b9a8e52fad35524a8035a","authors":["Jieshan Chen","Chunyang Chen","Zhenchang Xing","Xin Xia","Liming Zhu","John Grundy","Jinshui Wang"],"title":"Wireframe-based UI design search through image autoencoder","link":"https://arxiv.org/pdf/2103.07085","description":"UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for\xa0\u2026","yearPublished":"2020","citedBy":15,"category":{"type":"Journal","categoryTitle":"ACM Transactions on Software Engineering and Methodology (TOSEM)","pages":"1-31","publisher":"ACM","volume":"29","issue":"3"},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-29T14:32:46.040Z","updatedAt":"2021-08-29T14:32:46.040Z","year":2020},{"_id":"612cc3423c0bce8c741d7d82","authors":["Sa Gao","Chunyang Chen","Zhenchang Xing","Yukun Ma","Wen Song","Shang-Wei Lin"],"title":"A neural model for method name generation from functional description","link":"","description":"The names of software artifacts, e.g., method names, are important for software understanding and maintenance, as good names can help developers easily understand others\' code. However, the existing naming guidelines are difficult for developers, especially novices, to come up with meaningful, concise and compact names for the variables, methods, classes and files. With the popularity of open source, an enormous amount of project source code can be accessed, and the exhaustiveness and instability of manually naming methods could now be relieved by automatically learning a naming model from a large code repository. Nevertheless, building a comprehensive naming system is still challenging, due to the gap between natural language functional descriptions and method names. Specifically, there are three challenges: how to model the relationship between the functional descriptions and formal method\xa0\u2026","yearPublished":"2019","citedBy":17,"category":{"type":"Conference","categoryTitle":"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)","pages":"414-421","publisher":"IEEE","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-30T11:38:42.230Z","updatedAt":"2021-08-30T11:38:42.230Z","year":2019},{"_id":"612cc3423c0bce8c741d7d80","authors":["Xu Wang","Chunyang Chen","Zhenchang Xing"],"title":"Domain-specific machine translation with recurrent neural network for software localization","link":"","description":"Software localization is the process of adapting a software product to the linguistic, cultural and technical requirements of a target market. It allows software companies to access foreign markets that would be otherwise difficult to penetrate. Many studies have been carried out to locate need-to-translate strings in software and adapt UI layout after text translation in the new language. However, no work has been done on the most important and time-consuming step of software localization process, i.e., the translation of software text. Due to some unique characteristics of software text, for example, application-specific meanings, context-sensitive translation, domain-specific rare words, general machine translation tools such as Google Translate cannot properly address linguistic and technical nuance in translating software text for software localization. In this paper, we propose a neural-network based translation model\xa0\u2026","yearPublished":"2019","citedBy":16,"category":{"type":"Journal","categoryTitle":"Empirical Software Engineering","pages":"3514-3545","publisher":"Springer US","volume":"24","issue":"6"},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-30T11:38:42.229Z","updatedAt":"2021-08-30T11:38:42.229Z","year":2019},{"_id":"612cc3423c0bce8c741d7d83","authors":["Chunyang Chen","Zhenchang Xing","Yang Liu","Kent Long Xiong Ong"],"title":"Mining likely analogical apis across third-party libraries via large-scale unsupervised api semantics embedding","link":"","description":"Establishing API mappings between third-party libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined. Having an automatic technique to create a database of likely API mappings can significantly ease the task. Unfortunately, existing techniques either adopt supervised learning mechanism that requires already-ported or functionality similar applications across major programming languages or platforms, which are difficult to come by for an arbitrary pair of third-party libraries, or cannot deal with lexical gap in the API descriptions of different libraries. To overcome these limitations, we present an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. Based on\xa0\u2026","yearPublished":"2019","citedBy":34,"category":{"type":"Journal","categoryTitle":"IEEE Transactions on Software Engineering","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-30T11:38:42.230Z","updatedAt":"2021-08-30T11:38:42.230Z","year":2019},{"_id":"612cc3423c0bce8c741d7d81","authors":["Sen Chen","Lingling Fan","Chunyang Chen","Ting Su","Wenhe Li","Yang Liu","Lihua Xu"],"title":"Storydroid: Automated generation of storyboard for Android apps","link":"https://arxiv.org/pdf/1902.00476","description":"Mobile apps are now ubiquitous. Before developing a new app, the development team usually endeavors painstaking efforts to review many existing apps with similar purposes. The review process is crucial in the sense that it reduces market risks and provides inspiration for app development. However, manual exploration of hundreds of existing apps by different roles (e.g., product manager, UI/UX designer, developer) in a development team can be ineffective. For example, it is difficult to completely explore all the functionalities of the app in a short period of time. Inspired by the conception of storyboard in movie production, we propose a system, StoryDroid, to automatically generate the storyboard for Android apps, and assist different roles to review apps efficiently. Specifically, StoryDroid extracts the activity transition graph and leverages static analysis techniques to render UI pages to visualize the storyboard\xa0\u2026","yearPublished":"2019","citedBy":30,"category":{"type":"Conference","categoryTitle":"2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)","pages":"596-607","publisher":"IEEE","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-30T11:38:42.229Z","updatedAt":"2021-08-30T11:38:42.229Z","year":2019},{"_id":"612cc3423c0bce8c741d7d84","authors":["Simon Colton","Alison Pease","Michael Cook","Chunyang Chen"],"title":"The HR3 system for automated code generation in creative settings","link":"https://research.monash.edu/files/324969135/324817276_oa.pdf","description":"We describe the HR3 system for automated code generation, and its use in creative tasks. We outline the motivations and overall ideology behind its construction, most notably by identifying some distinctions in AI methodology which can be ignored when AI tasks are viewed as code generation problems to be solved. We further describe the nature of the approach in terms of: a programmatic interface to a Java API; production rule-based batch processing of data; on-demand code generation and inspection, and the usage of randomised and meta-level codebases. To support the claim that the approach is general purpose, we describe five applications in three areas normally covered by separate Computational Creativity systems, namely mathematical discovery, datamining and generative art. We end by discussing future directions for the HR3 system and how this project might address some higher-level issues in Computational Creativity.","yearPublished":"2019","citedBy":2,"category":{"type":"Journal","categoryTitle":"Proceedings of the 10th ICCC","pages":"","publisher":"","volume":"","issue":""},"teamId":"612b849beae47c89542bebca","__v":0,"createdAt":"2021-08-30T11:38:42.230Z","updatedAt":"2021-08-30T11:38:42.230Z","year":2019}]',REACT_APP_WEB_PAGES:'{"publicationOptions":{"layout":"All Publication","sortBy":"Title"},"pages":["PUBLICATIONS","TEAM"],"_id":"612b9be4536abc9ba4848519","teamId":"612b849beae47c89542bebca","createdAt":"2021-08-29T14:38:28.525Z","updatedAt":"2021-08-31T05:37:11.856Z","__v":1}'});f.REACT_APP_DEBUG?(console.log("Running in DEBUG mode, hence using fake Team data"),i=[{_id:"fake_publication_1",title:"Auto-icon: An automated code generation tool for icon designs assisting in ui development",description:'"Approximately 50% of development resources are devoted to UI development tasks [8]. Occupied a large proportion of development resources, developing icons can be a time-consuming task, because developers need to consider not only effective implementation methods but also easy-to-understand descriptions. In this study, we define 100 icon classes through an iterative open coding for the existing icon design sharing website. Based on a deep learning model and computer vision methods, we propose an approach to automatically convert icon images to fonts with descriptive labels, thereby reducing the laborious manual effort for developers and facilitating UI development. We quantitatively evaluate the quality of our method in the real world UI development environment and demonstrate that our method offers developers accurate, efficient, readable, and usable code for icon images, in terms of saving 65.2 \u2026',category:{type:"BOOK",categoryTitle:"26th International Conference on Intelligent User Interfaces",issue:"",volume:"",pages:"56-69",publisher:""},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Sidong Feng","Suyu Ma","Jinzhong Yu","Chunyang Chen","TingTing Zhou","Yankun Zhen"],yearPublished:"2021"},{_id:"fake_publication_2",title:"Context-aware Personalized Crowdtesting Task Recommendation",description:"Crowdsourced software testing (short for crowdtesting) is a special type of crowdsourcing. It requires that crowdworkers master appropriate skill-sets and commit significant effort for completing a task. Abundant uncertainty may arise during a crowdtesting process due to imperfect information between the task requester and crowdworkers. For example, a worker frequently chooses tasks in an ad hoc manner in crowdtesting context, and an inappropriate task selection may lead to the worker's failing to detect any bugs, and significant testing effort unpaid and wasted. Recent studies have explored methods for supporting task requesters to make informed decisions on task pricing, worker recommendation, and so on. Unfortunately, very few study offers decision making support from the crowdworkers' perspectives. We motivate this study through a pilot study, revealing the large portion (74%) of unpaid crowdworkers' \u2026",category:{type:"JOURNAL",categoryTitle:"International Economics Journal",issue:"10.1",volume:"11.2",pages:"10",publisher:"IMF"},link:"https://www.imf.org/en/Home",authors:["Jeremy Buffet","Warren Graham"],yearPublished:"2021"},{_id:"fake_publication_3",title:"MULTI-PHACET-MULTIdimensional clinical phenotyping of hospitalised acute COPD ExacerbaTions",description:"Securing generative Arts through NFTs Lorem ipsum amet dolor sit amet, minim altera mucius an eum. Etiam feugiat laoreet tempor. Vestibulum vel facilisis odio, in ultricies ex. Nullam vitae lectus vitae arcu efficitur auctor id a sem. Mauris congue enim risus, eu gravida mi dignissim ut. Vestibulum tempus urna vel sem eleifend, quis aliquet ligula maximus. Vivamus sagittis dolor eu iaculis interdum. Morbi ex odio, ornare eget erat eu, dapibus accumsan elit. Fusce fermentum orci ante. Etiam dolor urna, dictum a diam nec, ornare tempor nunc. Curabitur imperdiet malesuada augue eget vestibulum. Legere antiopam definitiones nam an.",category:{type:"BOOK",categoryTitle:"The digital asset certification",issue:"",volume:"",pages:"15",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Yoshua Benjio"],yearPublished:"2017"},{_id:"fake_publication_4",title:"CONCORDANCE OF LARYNGOSCOPY AND DYNAMIC COMPUTERIZED TOMOGRAPHY LARYNX TO DIAGNOSE VOCAL CORD DYSFUNCTION ",description:"",category:{type:"CONFERENCE",categoryTitle:"RESPIROLOGY",issue:"",volume:"",pages:"102-102",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Kaibo Cao","Chunyang Chen","Sebastian Baltes","Christoph Treude","Xiang Chen"],yearPublished:"2017"},{_id:"fake_publication_5",title:"Rivaroxaban compared to placebo for the treatment of leg superficial vein thrombosis: a randomized trial",description:"The role of rivaroxaban in the treatment of leg superficial venous thrombosis (SVT) is uncertain. This article aims to determine if rivaroxaban is an effective and safe treatment for leg SVT. Patients with symptomatic leg SVT of at least 5\u2009cm length were randomized to 45 days of rivaroxaban 10\u2009mg daily or to placebo, and followed for a total of 90 days. Treatment failure (required a nonstudy anticoagulant; had proximal deep vein thrombosis or pulmonary embolism; or had surgery for SVT) at 90 days was the primary efficacy outcome. Secondary efficacy outcomes included leg pain severity, and venous disease-specific and general health-related quality of life over 90 days. Major bleeding at 90 days was the primary safety outcome. Poor enrollment led to the trial being stopped after 85 of the planned 600 patients were randomized to rivaroxaban (n\u2009=\u200943) or placebo (n\u2009=\u200942). One rivaroxaban and five placebo \u2026",category:{type:"OTHER",categoryTitle:"",issue:"",volume:"54",pages:"15",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Yoshua Benjio"],yearPublished:"2017"},{_id:"fake_publication_6",title:"Owl Eyes: Spotting UI Display Issues via Visual Understanding",description:"Etiam feugiat laoreet tempor. Vestibulum vel facilisis odio, in ultricies ex. Nullam vitae lectus vitae arcu efficitur auctor id a sem. Mauris congue enim risus, eu gravida mi dignissim ut. Vestibulum tempus urna vel sem eleifend, quis aliquet ligula maximus. Vivamus sagittis dolor eu iaculis interdum. Morbi ex odio, ornare eget erat eu, dapibus accumsan elit. Fusce fermentum orci ante. Etiam dolor urna, dictum a diam nec, ornare tempor nunc. Curabitur imperdiet malesuada augue eget vestibulum.  Securing generative Arts through NFTs. Lorem ipsum amet dolor sit amet, minim altera mucius an eum. Legere antiopam definitiones nam an.",category:{type:"CONFERENCE",categoryTitle:"The digital asset certification",issue:"",volume:"",pages:"15",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Yoshua Benjio","Yoshua Aenjio","Yoshua Cenjio","Yoshua Denjio","Yoshua Eenjio","Yoshua Fenjio","Yoshua Genjio","Yoshua Henjio","Yoshua Ienjio"],yearPublished:"2017"},{_id:"fake_publication_7",title:"Wireframe-based UI design search through image autoencoder",description:"Securing generative Arts through NFTs",category:{type:"CONFERENCE",categoryTitle:"The digital asset certification",issue:"",volume:"",pages:"15",publisher:"Yoshua Benjio"},link:"thislinkisfakedonteventrytoclickit.okay",authors:["Yoshua Benjio"],yearPublished:"2017"}],n={twitterHandle:"elonmusk",orgName:"Monash",teamName:"MonTeam"},o=[{fullName:"John",position:"Chief Scientist",summary:"John is a chief scientist at MonTeam, working with the top government agencies to fight the pressing issues arising from climate change"},{fullName:"Yoshua Benjio",position:"Chief Data Scientist",summary:"Hailed as one of the founders of Deep Learning, Yoshua works at MonTeam to oversee strategic deep learning project designs"},{fullName:"Jeremy Buffet",position:"Chief Economist",summary:"Jeremy Buffet leads our macro-economic unit in predicting macro factors and their impact on society"}],s={aboutUs:["This is first paragraph.","This is second paragraph.","This is third paragraph."]},r={pages:["PUBLICATIONS","TEAM"],publicationOptions:{layout:"All Publication",sortBy:"Author"}}):(i=f.REACT_APP_TEAM_PUBLICATIONS?JSON.parse(f.REACT_APP_TEAM_PUBLICATIONS):[],n=f.REACT_APP_TEAM_INFO?JSON.parse(f.REACT_APP_TEAM_INFO):null,o=f.REACT_APP_TEAM_MEMBERS?JSON.parse(f.REACT_APP_TEAM_MEMBERS):[],s=f.REACT_APP_TEAM_HOMEPAGE?JSON.parse(f.REACT_APP_TEAM_HOMEPAGE):null,r=f.REACT_APP_WEB_PAGES?JSON.parse(f.REACT_APP_WEB_PAGES):[]);var A=a(37),b=a(64),y=a(38),v=a(67),w=a(1),I=function(e,t){var a=Object(c.useState)(1),i=Object(y.a)(a,2),n=i[0],o=i[1],s=Math.ceil(e.length/t),r=function(){o((function(e){return Math.min(e+1,s)}))},l=function(){o((function(e){return Math.max(e-1,1)}))};return{currentData:function(){if(e){var a=(n-1)*t,i=a+t;return e.slice(a,i)}},pagination:function(){for(var a=[],i=function(e){a.push(Object(w.jsx)(v.a.Item,{onClick:function(){return function(e){var t=Math.max(1,e);o(Math.min(t,s))}(e)},active:e===n,children:e},e))},c=1;c<=s;c++)i(c);return e.length>t&&Object(w.jsx)("div",{style:{display:"flex",justifyContent:"center"},children:Object(w.jsxs)(v.a,{children:[Object(w.jsx)(v.a.Prev,{onClick:l,disabled:1===n}),a,Object(w.jsx)(v.a.Next,{onClick:r,disabled:n===s})]})})}}},T=a(66),k=function(e){var t=e.pub;return Object(w.jsxs)(T.a,{className:"publication-card",children:[Object(w.jsxs)(b.a.Toggle,{as:T.a.Header,eventKey:t._id,className:"publication-title-column",children:[Object(w.jsx)("div",{className:"pub-category-above-title",children:t.category.type}),Object(w.jsxs)("div",{className:"publication-title",children:[" ",t.title]}),Object(w.jsxs)("div",{className:"pub-year-below-title",children:[" ",t.yearPublished," "]})]}),Object(w.jsx)(b.a.Collapse,{eventKey:t._id,children:Object(w.jsxs)(T.a.Body,{className:"publication-body-column",children:[Object(w.jsx)("div",{className:"pub-body-subheader",children:"Authors"}),Object(w.jsx)("div",{className:"pub-body-content",children:t.authors.map((function(e){return"".concat(e)})).join(", ")}),Object(w.jsx)("div",{className:"pub-body-subheader",children:"Description"}),Object(w.jsx)("div",{className:"pub-body-content pub-body-paragraph",children:t.description}),Object(w.jsx)("div",{className:"pub-body-subheader",children:t.category.categoryTitle?t.category.type.charAt(0)+t.category.type.slice(1).toLowerCase():""}),Object(w.jsx)("div",{className:"pub-body-content",children:t.category.categoryTitle?t.category.categoryTitle+(t.category.issue?", Issue "+t.category.issue:"")+(t.category.volume?", Volume "+t.category.volume:"")+(t.category.pages?", Page "+t.category.pages:""):""}),Object(w.jsx)("div",{className:"pub-body-subheader",children:t.category.publisher?"Published by":null}),Object(w.jsx)("div",{className:"pub-body-content",children:t.category.publisher}),Object(w.jsx)("div",{className:"pub-body-subheader",children:t.link?"View at":null}),Object(w.jsx)("div",{className:"pub-body-content",children:Object(w.jsx)("a",{href:t.link,children:t.link})})]})})]})},C=a(14),x=function(e){var t=e.teamPublications,a=e.pageSize,i=I(t,a||C.pageSize),n=i.currentData,o=i.pagination;return Object(w.jsxs)("div",{className:"mb-5",children:[n().map((function(e){return Object(w.jsx)(k,{pub:e},e._id)})),o()]})},E=function(e){var t=e.teamPublications,a=function(e){var a=t.filter((function(t){return t.category.type===e}));return a.length>0&&Object(w.jsxs)(w.Fragment,{children:[Object(w.jsxs)("h3",{className:"text-center",children:[" ",e," "]}),Object(w.jsx)(x,{teamPublications:a,pageSize:C.categoryPageSize})]})};return Object.keys(C.categoryType).map((function(e,t){return Object(w.jsx)("div",{children:a(e)},t)}))},j=function(){var e,t=null!==(e=r.publicationOptions)&&void 0!==e?e:C.defaultOption,a=function(e,t){switch(t){case C.sortingOptions.AUTHOR:e.sort((function(e,t){return e.authors[0].toLowerCase()>t.authors[0].toLowerCase()?1:e.authors[0].toLowerCase()<t.authors[0].toLowerCase()?-1:0}));break;case C.sortingOptions.TITLE:e.sort((function(e,t){return e.title.toLowerCase()>t.title.toLowerCase()?1:e.title.toLowerCase()<t.title.toLowerCase()?-1:0}));break;case"Category Title":e.sort((function(e,t){return e.category.categoryTitle.toLowerCase()>t.category.categoryTitle.toLowerCase()?1:e.category.categoryTitle.toLowerCase()<t.category.categoryTitle.toLowerCase()?-1:0}));break;default:e.sort((function(e,t){return e.title.toLowerCase()>t.title.toLowerCase()?1:e.title.toLowerCase()<t.title.toLowerCase()?-1:0})),e.sort((function(e,t){return e.year>t.year?-1:e.year<t.year?1:0}))}return e}(i,t.sortBy);return Object(w.jsx)(c.Fragment,{children:Object(w.jsx)(b.a,{children:function(){switch(t.layout){case C.layoutOptions.BY_CATEGORY:return Object(w.jsx)(E,{teamPublications:a});default:return Object(w.jsx)(x,{teamPublications:a})}}()})})},O=function(){return Object(w.jsxs)(c.Fragment,{children:[Object(w.jsx)(g.a,{className:"pages-top-padding text-center mt-3 mb-3",children:Object(w.jsx)("div",{className:"publication-pg-title",children:"Our Publications"})}),Object(w.jsx)(g.a,{fluid:!0,children:Object(w.jsx)(j,{})})]})},U=function(){var e=s;return Object(w.jsx)(c.Fragment,{children:Object(w.jsxs)(g.a,{fluid:!0,className:"pages-top-padding",children:[Object(w.jsx)("div",{className:"landing-center-title",children:"About Us"}),e.aboutUs.map((function(e){return Object(w.jsx)("div",{className:"landing-center-content",children:e})}))]})})},P=a(62),_=function(e){var t=e.member;return Object(w.jsxs)(T.a,{className:"team-card",children:[Object(w.jsx)(T.a.Img,{variant:"top",src:"data:image/jpeg;base64,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"}),Object(w.jsxs)(T.a.Body,{children:[Object(w.jsx)("div",{className:"member-name",children:t.fullName}),Object(w.jsx)("div",{className:"member-position",children:t.position}),Object(w.jsx)("div",{className:"member-summary",children:t.summary})]})]})},S=[{title:"Publications",path:"/publication",exact:!0,component:O},{title:"Team",path:"/team",exact:!0,component:function(){var e=o;return Object(w.jsxs)(c.Fragment,{children:[Object(w.jsx)(g.a,{className:"pages-top-padding text-center mt-3 mb-3",children:Object(w.jsx)("div",{className:"team-pg-title",children:"Meet Our Team"})}),Object(w.jsx)(g.a,{fluid:!0,className:"team-card-container",children:Object(w.jsx)(P.a,{className:"team-card-deck",children:e.map((function(e){return Object(w.jsx)(_,{member:e},e._id)}))})})]})}}],N=[{title:"Home",path:"/",exact:!0,component:U}],B=function(){var e=S.filter((function(e){var t=e.title;return r.pages.includes(t.toUpperCase())}));return[].concat(N,Object(A.a)(e))},G=function(){var e=n.orgName,t=n.teamName,a=B();return console.log(a),Object(w.jsx)(c.Fragment,{children:Object(w.jsx)(h.a,{collapseOnSelect:!0,expand:"md",bg:"light",variant:"light",fixed:"top",children:Object(w.jsxs)(g.a,{fluid:!0,children:[Object(w.jsxs)(h.a.Brand,{as:u.b,to:"/",children:[t," @ ",e]}),Object(w.jsx)(h.a.Toggle,{"aria-controls":"responsive-navbar-nav"}),Object(w.jsxs)(h.a.Collapse,{id:"responsive-navbar-nav",children:[Object(w.jsx)(m.a,{className:"me-auto"}),Object(w.jsx)(m.a,{children:a.map((function(e,t){var a=e.path,i=e.title;return Object(w.jsx)(m.a.Link,{as:u.b,to:a,children:i},t)}))})]})]})})})},M=(a(59),function(){var e=B().map((function(e){var t=e.path,a=e.exact,i=e.component;return Object(w.jsx)(p.a,{exact:a,path:t,children:Object(w.jsx)("div",{children:i?Object(w.jsx)(i,{}):null})},t)}));return Object(w.jsxs)(c.Fragment,{children:[Object(w.jsx)(G,{}),Object(w.jsx)(p.c,{children:e})]})});d.a.render(Object(w.jsx)(u.a,{children:Object(w.jsx)(M,{})}),document.getElementById("root"))}},[[60,1,2]]]);
//# sourceMappingURL=main.8426c71e.chunk.js.map