(this.webpackJsonpbase=this.webpackJsonpbase||[]).push([[0],{28:function(e,a,t){},47:function(e,a,t){},57:function(e,a,t){},58:function(e,a,t){},59:function(e,a,t){"use strict";t.r(a);var i=t(1),n=t(16),o=t.n(n),s=t(13),r=(t(42),t(43),t(44),t(45),t(32)),d=t(31),l=t(24),c=t(36),h=(t(47),t(2)),g=function(e){var a=e.linkedHandle;return Object(h.jsx)("div",{className:"twitter-feed",children:Object(h.jsx)(c.a,{dataSource:{sourceType:"profile",screenName:a},options:{height:800}})})},u=t(35),p=t(34),f=t(62),m=t(17),y=t(23),b=t(0),v=(t(28),function(e){var a=e.pub,t=Object(i.useState)(!1),n=Object(u.a)(t,2),o=n[0],s=n[1],r=Object(h.jsx)(p.a,{in:o,children:Object(h.jsxs)("div",{style:{marginLeft:"15px",marginRight:"10px",marginBottom:"15px"},children:[Object(h.jsxs)("h5",{children:[Object(h.jsx)("b",{children:"Description:"})," ",a.description]}),Object(h.jsxs)("h5",{children:[Object(h.jsxs)("b",{children:[a.category.type.charAt(0)+a.category.type.slice(1).toLowerCase(),":"]})," ",a.category.categoryTitle]}),a.category.issue&&Object(h.jsxs)("h5",{children:[" ",Object(h.jsx)("b",{children:"Issue:"})," ",a.category.issue," "]}),a.category.volume&&Object(h.jsxs)("h5",{children:[" ",Object(h.jsx)("b",{children:"Volume:"})," ",a.category.volume," "]}),a.category.pages&&Object(h.jsxs)("h5",{children:[Object(h.jsx)("b",{children:"Pages:"})," ",a.category.pages," "]}),a.category.publisher&&Object(h.jsxs)("h5",{children:[" ",Object(h.jsx)("b",{children:"Publisher:"})," ",a.category.publisher," "]}),Object(h.jsxs)(d.a,{children:[Object(h.jsx)(l.a,{md:11,children:a.link&&Object(h.jsx)(f.a,{onClick:function(){return window.open("".concat(a.link),"_blank")},children:Object(h.jsx)(b.b.Provider,{value:{color:"black",size:"25px"},children:Object(h.jsx)(m.c,{})})})}),Object(h.jsx)(l.a,{md:1,children:Object(h.jsx)("span",{onClick:function(){return s(!o)},children:o&&Object(h.jsx)(b.b.Provider,{value:{color:"black",size:"25px"},children:Object(h.jsx)(y.b,{style:{marginLeft:"10px"}})})})})]})]})});return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)("div",{className:"modalHeader",children:Object(h.jsx)(d.a,{children:Object(h.jsx)(l.a,{md:11,children:Object(h.jsx)("h3",{style:{marginLeft:"15px",marginTop:"15px"},children:a.title})})})}),Object(h.jsxs)("div",{style:{marginLeft:"15px"},className:o?"mt-3":"mt-3 mb-2",children:[Object(h.jsxs)("h5",{children:[Object(h.jsx)("b",{children:" Authors: "}),a.authors.map((function(e){return"".concat(e)})).join(", ")]}),Object(h.jsxs)(d.a,{children:[Object(h.jsx)(l.a,{md:11,children:Object(h.jsxs)("h5",{className:o?"":"blur",children:[Object(h.jsx)("b",{children:"Year Published: "}),a.yearPublished," "]})}),Object(h.jsx)(l.a,{md:1,children:Object(h.jsx)("span",{onClick:function(){return s(!o)},children:!o&&Object(h.jsx)(b.b.Provider,{value:{color:"black",size:"25px"},children:Object(h.jsx)(y.a,{style:{marginLeft:"10px"}})})})})]})]}),r]})}),w=JSON.parse('[{"_id":"6107aafe5708e15cf0c20625","authors":["Bo Yang","Zhenchang Xing","Xin Xia","Chunyang Chen","Deheng Ye","Shanping Li"],"title":"Don\u2019t Do That! Hunting Down Visual Design Smells in Complex UIs against Design Guidelines","link":"","description":"Just like code smells in source code, UI design has visual design smells. We study 93 don\u2019t-do-that guidelines in the Material Design, a complex design system created by Google. We find that these don\u2019t-guidelines go far beyond UI aesthetics, and involve seven general design dimensions (layout, typography, iconography, navigation, communication, color, and shape) and four component design aspects (anatomy, placement, behavior, and usage). Violating these guidelines results in visual design smells in UIs (or UI design smells). In a study of 60,756 UIs of 9,286 Android apps, we find that 7,497 UIs of 2,587 apps have at least one violation of some Material Design guidelines. This reveals the lack of developer training and tool support to avoid UI design smells. To fill this gap, we design an automated UI design smell detector (UIS-Hunter) that extracts and validates multi-modal UI information (component\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"CONFERENCE","categoryTitle":"2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)","pages":"761-772","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-02T08:21:18.665Z","updatedAt":"2021-08-02T08:21:18.665Z","year":2021},{"_id":"610dff68da292ca87c707f5a","authors":["Tianming Zhao","Chunyang Chen","Yuanning Liu","Xiaodong Zhu"],"title":"GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Networks","link":"https://arxiv.org/pdf/2101.09978","description":"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications, and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. Besides, the requirement of the rapid development of GUI design also aggravates designers\u2019 working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model GUIGAN to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our GUIGAN is to reuse GUI components collected from existing mobile app GUIs for composing a new design that is similar to natural-language generation. Our\xa0\u2026","yearPublished":"2021","citedBy":1,"category":{"type":"CONFERENCE","categoryTitle":"43rd International Conference on Software Engineering (ICSE)","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.908Z","updatedAt":"2021-08-07T03:35:04.908Z","year":2021},{"_id":"6107aafe5708e15cf0c20627","authors":["Qiuyuan Chen","Chunyang Chen","Safwat Hassan","Zhengchang Xing","Xin Xia","Ahmed E Hassan"],"title":"How Should I Improve the UI of My App? A Study of User Reviews of Popular Apps in the Google Play","link":"","description":"UI (User Interface) is an essential factor influencing users\u2019 perception of an app. However, it is hard for even professional designers to determine if the UI is good or not for end-users. Users\u2019 feedback (e.g., user reviews in the Google Play) provides a way for app owners to understand how the users perceive the UI. In this article, we conduct an in-depth empirical study to analyze the UI issues of mobile apps. In particular, we analyze more than 3M UI-related reviews from 22,199 top free-to-download apps and 9,380 top non-free apps in the Google Play Store. By comparing the rating of UI-related reviews and other reviews of an app, we observe that UI-related reviews have lower ratings than other reviews. By manually analyzing a random sample of 1,447 UI-related reviews with a 95% confidence level and a 5% interval, we identify 17 UI-related issues types that belong to four categories (i.e., \u201cAppearance\xa0\u2026","yearPublished":"2021","citedBy":2,"category":{"type":"JOURNAL","categoryTitle":"ACM Transactions on Software Engineering and Methodology (TOSEM)","pages":"1-38","publisher":"ACM","volume":"30","issue":"3"},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-02T08:21:18.666Z","updatedAt":"2021-08-02T08:21:18.666Z","year":2021},{"_id":"610dff68da292ca87c707f5c","authors":["Yujin Huang","Han Hu","Chunyang Chen"],"title":"Robustness of on-device Models: Adversarial Attack to Deep Learning Models on Android Apps","link":"https://arxiv.org/pdf/2101.04401","description":"Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models\u2019 exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the\xa0\u2026","yearPublished":"2021","citedBy":5,"category":{"type":"CONFERENCE","categoryTitle":"43rd International Conference on Software Engineering","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.909Z","updatedAt":"2021-08-07T03:35:04.909Z","year":2021},{"_id":"6107aafe5708e15cf0c20624","authors":["Bo Yang","Zhenchang Xing","Xin Xia","Chunyang Chen","Deheng Ye","Shanping Li"],"title":"UIS-Hunter: Detecting UI Design Smells in Android Apps","link":"","description":"Similar to code smells in source code, UI design has visual design smells that indicate violations of good UI design guidelines. UI design guidelines constitute design systems for a vast variety of products, platforms, and services. Following a design system, developers can avoid common design issues and pitfalls. However, a design system is often complex, involving various design dimensions and numerous UI components. Lack of concerns on GUI visual effect results in little support for detecting UI design smells that violate the design guidelines in a complex design system. In this paper, we propose an automated UI design smell detector named UIS-Hunter (UI design Smell Hunter). The tool is able to (i) automatically process UI screenshots or prototype files to detect UI design smells and generate reports, (ii) highlight the violated UI regions and list the material design guidelines that the found design smells\xa0\u2026","yearPublished":"2021","citedBy":null,"category":{"type":"CONFERENCE","categoryTitle":"2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","pages":"89-92","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-02T08:21:18.665Z","updatedAt":"2021-08-02T08:21:18.665Z","year":2021},{"_id":"60fb4d005ff50499387c7c01","authors":["cwecew"],"title":"csdcwcwe","yearPublished":"2021","description":"cweccewc","link":"","category":{"type":"JOURNAL","categoryTitle":"cwecewcew","volume":"","issue":"","pages":"","publisher":""},"teamId":"60f00ad1f419ce20b8bd930d","createdAt":"2021-07-23T23:13:04.554Z","updatedAt":"2021-07-23T23:13:04.554Z","__v":0,"year":2021},{"_id":"60fb46b3baacea78a0486c0d","authors":["dwedwed"],"title":"dwedwedewd 123","yearPublished":"2021","description":"dwedwedewd","link":"","category":{"type":"JOURNAL","categoryTitle":"dwedwedwe","volume":"","issue":"","pages":"","publisher":""},"teamId":"60f00ad1f419ce20b8bd930d","createdAt":"2021-07-23T22:46:11.305Z","updatedAt":"2021-07-23T22:50:51.651Z","__v":0,"year":2021},{"_id":"60f93e39d82c5584c8d9c1e8","authors":["fwefwe"],"title":"fwefewf","yearPublished":"2021","description":"fewfweffwewf","link":"","category":{"type":"JOURNAL","categoryTitle":"fwefwefew","volume":"","issue":"","pages":"","publisher":""},"teamId":"60f00ad1f419ce20b8bd930d","createdAt":"2021-07-22T09:45:29.637Z","updatedAt":"2021-07-22T09:45:29.637Z","__v":0,"year":2021},{"_id":"610dff68da292ca87c707f5d","authors":["Han Wang","Chunyang Chen","Zhenchang Xing","John Grundy"],"title":"DiffTech: A tool for differencing similar technologies from question-and-answer discussions","link":"","description":"Developers can use different technologies for different software development tasks in their work. However, when faced with several technologies with comparable functionalities, it can be challenging for developers to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers resort to expert articles, read official documents or ask questions in Q&A sites for technology comparison. However, it is still very opportunistic whether they will get a comprehensive comparison, as online information is often fragmented, contradictory and biased. To overcome these limitations, we propose the DiffTech system that exploits the crowd sourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We found 19,118 comparative sentences from 2,410 pairs of comparable technologies\xa0\u2026","yearPublished":"2020","citedBy":1,"category":{"type":"BOOK","categoryTitle":"Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"1576-1580","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.909Z","updatedAt":"2021-08-07T03:35:04.909Z","year":2020},{"_id":"610dff68da292ca87c707f60","authors":["Chunyang Chen","Sidong Feng","Zhengyang Liu","Zhenchang Xing","Shengdong Zhao"],"title":"From Lost to Found: Discover Missing UI Design Semantics through Recovering Missing Tags","link":"https://dl.acm.org/doi/pdf/10.1145/3415194","description":"Design sharing sites provide UI designers with a platform to share their works and also an opportunity to get inspiration from others\' designs. To facilitate management and search of millions of UI design images, many design sharing sites adopt collaborative tagging systems by distributing the work of categorization to the community. However, designers often do not know how to properly tag one design image with compact textual description, resulting in unclear, incomplete, and inconsistent tags for uploaded examples which impede retrieval, according to our empirical study and interview with four professional designers. Based on a deep neural network, we introduce a novel approach for encoding both the visual and textual information to recover the missing tags for existing UI examples so that they can be more easily found by text queries. We achieve 82.72% accuracy in the tag prediction. Through a simulation\xa0\u2026","yearPublished":"2020","citedBy":9,"category":{"type":"JOURNAL","categoryTitle":"Proceedings of the ACM on Human-Computer Interaction","pages":"123:1-123:22","publisher":"ACM","volume":"4","issue":"CSCW2"},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.909Z","updatedAt":"2021-08-07T03:35:04.909Z","year":2020},{"_id":"610dff68da292ca87c707f61","authors":["Jieshan Chen","Mulong Xie","Zhenchang Xing","Chunyang Chen","Xiwei Xu","Liming Zhu"],"title":"Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination?","link":"https://arxiv.org/pdf/2008.05132","description":"Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task. It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation. Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (eg, canny edge, contours), and deep learning models that learn to detect from large-scale GUI data. Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task. We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these\xa0\u2026","yearPublished":"2020","citedBy":18,"category":{"type":"CONFERENCE","categoryTitle":"(ESEC/FSE) ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.910Z","updatedAt":"2021-08-07T03:35:04.910Z","year":2020},{"_id":"610dff68da292ca87c707f5f","authors":["Zhe Liu","Chunyang Chen","Junjie Wang","Yuekai Huang","Jun Hu","Qing Wang"],"title":"Owl Eyes: Spotting UI Display Issues via Visual Understanding","link":"https://arxiv.org/pdf/2009.01417","description":"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the development of technology and aesthetics, the visual effects of the GUI are more and more attracting. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding\xa0\u2026","yearPublished":"2020","citedBy":6,"category":{"type":"CONFERENCE","categoryTitle":"2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.909Z","updatedAt":"2021-08-07T03:35:04.909Z","year":2020},{"_id":"610dff68da292ca87c707f64","authors":["Dehai Zhao","Zhenchang Xing","Chunyang Chen","Xiwei Xu","Liming Zhu","Guoqiang Li","Jinshui Wang"],"title":"Seenomaly: vision-based linting of GUI animation effects against design-don\'t guidelines","link":"","description":"GUI animations, such as card movement, menu slide in/out, snack-bar display, provide appealing user experience and enhance the usability of mobile applications. These GUI animations should not violate the platform\'s UI design guidelines (referred to as design-don\'t guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes. However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can \u201csee\u201d the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don\'t guidelines. In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier. Instead, we propose an unsupervised, computer-vision based adversarial\xa0\u2026","yearPublished":"2020","citedBy":14,"category":{"type":"CONFERENCE","categoryTitle":"42nd International Conference on Software Engineering (ICSE\u201920). ACM, New York, NY","pages":"1286 - 1297","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.910Z","updatedAt":"2021-08-07T03:35:04.910Z","year":2020},{"_id":"610dff68da292ca87c707f65","authors":["Chunyang Chen"],"title":"SimilarAPI: Mining Analogical APIs for Library Migration","link":"","description":"Establishing API mappings between libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined, and existing methods based on supervised learning requires unavailable already-ported or functionality similar applications. Therefore, we propose an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. We implement a proof-of-concept website SimilarAPI (https://similarapi.appspot.com) which can recommend analogical APIs for 583,501 APIs of 111 pairs of analogical Java libraries with diverse functionalities. Video: https://youtu.be/EAwD6l24vLQ","yearPublished":"2020","citedBy":3,"category":{"type":"CONFERENCE","categoryTitle":"International Conference on Software Engineering (ICSE), Demo Track","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.910Z","updatedAt":"2021-08-07T03:35:04.910Z","year":2020},{"_id":"610dff68da292ca87c707f5e","authors":["Mulong Xie","Sidong Feng","Zhenchang Xing","Jieshan Chen","Chunyang Chen"],"title":"UIED: a hybrid tool for GUI element detection","link":"https://www.researchgate.net/profile/Mulong-Xie/publication/346170544_UIED_a_hybrid_tool_for_GUI_element_detection/links/5fbca13a458515b797641b72/UIED-a-hybrid-tool-for-GUI-element-detection.pdf","description":"Graphical User Interface (GUI) elements detection is critical for many GUI automation and GUI testing tasks. Acquiring the accurate positions and classes of GUI elements is also the very first step to conduct GUI reverse engineering or perform GUI testing. In this paper, we implement a User Iterface Element Detection (UIED), a toolkit designed to provide user with a simple and easy-to-use platform to achieve accurate GUI element detection. UIED integrates multiple detection methods including old-fashioned computer vision (CV) approaches and deep learning models to handle diverse and complicated GUI images. Besides, it equips with a novel customized GUI element detection methods to produce state-of-the-art detection results. Our tool enables the user to change and edit the detection result in an interactive dashboard. Finally, it exports the detected UI elements in the GUI image to design files that can be\xa0\u2026","yearPublished":"2020","citedBy":4,"category":{"type":"BOOK","categoryTitle":"Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering","pages":"1655-1659","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.909Z","updatedAt":"2021-08-07T03:35:04.909Z","year":2020},{"_id":"610dff68da292ca87c707f63","authors":["Jieshan Chen","Chunyang Chen","Zhenchang Xing","Xiwei Xu","Liming Zhu","Guoqiang Li","Jinshui Wang"],"title":"Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning","link":"https://arxiv.org/pdf/2003.00380","description":"According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users\' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android\xa0\u2026","yearPublished":"2020","citedBy":25,"category":{"type":"CONFERENCE","categoryTitle":"42nd International Conference on Software Engineering (ICSE\u201920)","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.910Z","updatedAt":"2021-08-07T03:35:04.910Z","year":2020},{"_id":"610dff68da292ca87c707f62","authors":["Jieshan Chen","Chunyang Chen","Zhenchang Xing","Xin Xia","Liming Zhu","John Grundy","Jinshui Wang"],"title":"Wireframe-based UI design search through image autoencoder","link":"https://arxiv.org/pdf/2103.07085","description":"UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for\xa0\u2026","yearPublished":"2020","citedBy":12,"category":{"type":"JOURNAL","categoryTitle":"ACM Transactions on Software Engineering and Methodology (TOSEM)","pages":"1-31","publisher":"ACM","volume":"29","issue":"3"},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.910Z","updatedAt":"2021-08-07T03:35:04.910Z","year":2020},{"_id":"610dff68da292ca87c707f6c","authors":["Sa Gao","Chunyang Chen","Zhenchang Xing","Yukun Ma","Wen Song","Shang-Wei Lin"],"title":"A neural model for method name generation from functional description","link":"","description":"The names of software artifacts, e.g., method names, are important for software understanding and maintenance, as good names can help developers easily understand others\' code. However, the existing naming guidelines are difficult for developers, especially novices, to come up with meaningful, concise and compact names for the variables, methods, classes and files. With the popularity of open source, an enormous amount of project source code can be accessed, and the exhaustiveness and instability of manually naming methods could now be relieved by automatically learning a naming model from a large code repository. Nevertheless, building a comprehensive naming system is still challenging, due to the gap between natural language functional descriptions and method names. Specifically, there are three challenges: how to model the relationship between the functional descriptions and formal method\xa0\u2026","yearPublished":"2019","citedBy":17,"category":{"type":"CONFERENCE","categoryTitle":"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)","pages":"414-421","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f70","authors":["Dehai Zhao","Zhenchang Xing","Chunyang Chen","Xin Xia","Guoqiang Li"],"title":"ActionNet: Vision-based Workflow Action Recognition From Programming Screencasts","link":"","description":"Programming screencasts have two important applications in software engineering context: study developer behaviors, information needs and disseminate software engineering knowledge. Although programming screencasts are easy to produce, they are not easy to analyze or index due to the image nature of the data. Existing techniques extract only content from screencasts, but ignore workflow actions by which developers accomplish programming tasks. This significantly limits the effective use of programming screencasts in downstream applications. In this paper, we are the first to present a novel technique for recognizing workflow actions in programming screencasts. Our technique exploits image differencing and Convolutional Neural Network (CNN) to analyze the correspondence and change of consecutive frames, based on which nine classes of frequent developer actions can be recognized from\xa0\u2026","yearPublished":"2019","citedBy":14,"category":{"type":"CONFERENCE","categoryTitle":"ACM/IEEE International Conference on Software Engineering (ICSE)","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f66","authors":["Xu Wang","Chunyang Chen","Zhenchang Xing"],"title":"Domain-specific machine translation with recurrent neural network for software localization","link":"","description":"Software localization is the process of adapting a software product to the linguistic, cultural and technical requirements of a target market. It allows software companies to access foreign markets that would be otherwise difficult to penetrate. Many studies have been carried out to locate need-to-translate strings in software and adapt UI layout after text translation in the new language. However, no work has been done on the most important and time-consuming step of software localization process, i.e., the translation of software text. Due to some unique characteristics of software text, for example, application-specific meanings, context-sensitive translation, domain-specific rare words, general machine translation tools such as Google Translate cannot properly address linguistic and technical nuance in translating software text for software localization. In this paper, we propose a neural-network based translation model\xa0\u2026","yearPublished":"2019","citedBy":16,"category":{"type":"JOURNAL","categoryTitle":"Empirical Software Engineering","pages":"3514-3545","publisher":"Springer US","volume":"24","issue":"6"},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.910Z","updatedAt":"2021-08-07T03:35:04.910Z","year":2019},{"_id":"610dff68da292ca87c707f69","authors":["Suyu Ma","Zhenchang Xing","Chunyang Chen","Cheng Chen","Lizhen Qu","Guoqiang Li"],"title":"Easy-to-deploy API extraction by multi-level feature embedding and transfer learning","link":"","description":"Application Programming Interfaces (APIs) have been widely discussed on social-technical platforms (e.g., Stack Overflow). Extracting API mentions from such informal software texts is the prerequisite for API-centric search and summarization of programming knowledge. Machine learning based API extraction has demonstrated superior performance than rule-based methods in informal software texts that lack consistent writing forms and annotations. However, machine learning based methods have a significant overhead in preparing training data and effective features. In this paper, we propose a multi-layer neural network based architecture for API extraction. Our architecture automatically learns character-, word- and sentence-level features from the input texts, thus removing the need for manual feature engineering and the dependence on advanced features (e.g., API gazzetter) beyond the input texts. We also\xa0\u2026","yearPublished":"2019","citedBy":14,"category":{"type":"JOURNAL","categoryTitle":"IEEE Transactions on Software Engineering","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f68","authors":["Chunyang Chen","Sidong Feng","Zhenchang Xing","Linda Liu","Shengdong Zhao","Jinshui Wang"],"title":"Gallery D.C.: Design Search and Knowledge Discovery through Auto-created GUI Component Gallery","link":"https://nus-hci.org/wp-content/uploads/publications/2019/Chen%20et%20al.%20-%202019%20-%20Gallery%20D.C.%20Design%20Search%20and%20Knowledge%20Discover.pdf","description":"Online communities like Dribbble and GraphicBurger allow GUI designers to share their design artwork and learn from each other. These design sharing platforms are important sources for design inspiration, but our survey with GUI designers suggests additional information needs unmet by existing design sharing platforms. First, designers need to see the practical use of certain GUI designs in real applications, rather than just artworks. Second, designers want to see not only the overall designs but also the detailed design of the GUI components. Third, designers need advanced GUI design search abilities (e.g., multi-facets search) and knowledge discovery support (e.g., demographic investigation, cross-company design comparison). This paper presents Gallery D.C. http://mui-collection.herokuapp.com/, a gallery of GUI design components that harness GUI designs crawled from millions of real-world applications\xa0\u2026","yearPublished":"2019","citedBy":17,"category":{"type":"JOURNAL","categoryTitle":"Proceedings of the ACM on Human-Computer Interaction","pages":"180:1-180:22","publisher":"ACM","volume":"3","issue":"CSCW"},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f67","authors":["Sen Chen","Lingling Fan","Chunyang Chen","Minhui Xue","Yang Liu","Lihua Xu"],"title":"Gui-squatting attack: Automated generation of android phishing apps","link":"","description":"Mobile phishing attacks, such as mimic mobile browser pages, masquerade as legitimate applications by leveraging repackaging or clone techniques, have caused varied yet significant security concerns. Consequently, detection techniques have been receiving increasing attention. However, many such detection methods are not well tested and may therefore still be vulnerable to new types of phishing attacks. In this paper, we propose a new attacking technique, named GUI-Squatting attack, which can generate phishing apps (phapps) automatically and effectively. Our method adopts image processing and deep learning algorithms, to enable powerful and large-scale attacks. We observe that a successful phishing attack requires two conditions, page confusion and logic deception during attacks synthesis. We directly optimize these two conditions to create a practical attack. Our experimental results reveal that\xa0\u2026","yearPublished":"2019","citedBy":21,"category":{"type":"JOURNAL","categoryTitle":"IEEE Transactions on Dependable and Secure Computing","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.910Z","updatedAt":"2021-08-07T03:35:04.910Z","year":2019},{"_id":"610dff68da292ca87c707f6d","authors":["Chunyang Chen","Zhenchang Xing","Yang Liu","Kent Long Xiong Ong"],"title":"Mining likely analogical apis across third-party libraries via large-scale unsupervised api semantics embedding","link":"","description":"Establishing API mappings between third-party libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined. Having an automatic technique to create a database of likely API mappings can significantly ease the task. Unfortunately, existing techniques either adopt supervised learning mechanism that requires already-ported or functionality similar applications across major programming languages or platforms, which are difficult to come by for an arbitrary pair of third-party libraries, or cannot deal with lexical gap in the API descriptions of different libraries. To overcome these limitations, we present an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. Based on\xa0\u2026","yearPublished":"2019","citedBy":32,"category":{"type":"JOURNAL","categoryTitle":"IEEE Transactions on Software Engineering","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f6f","authors":["Xiang Chen","Chunyang Chen","Zhang Dun","Zhenchang Xing"],"title":"SEthesaurus: WordNet in Software Engineering","link":"","description":"Informal discussions on social platforms (e.g., Stack Overflow, CodeProject) have accumulated a large body of programming knowledge in the form of natural language text. Natural language process (NLP) techniques can be utilized to harvest this knowledge base for software engineering tasks. However, consistent vocabulary for a concept is essential to make an effective use of these NLP techniques. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms (such as abbreviations, synonyms and misspellings) in informal discussions. Existing techniques to deal with such morphological forms are either designed for general English or mainly resort to domain-specific lexical rules. A thesaurus, which contains software-specific terms and commonly-used morphological forms, is desirable to perform normalization for software engineering text. However\xa0\u2026","yearPublished":"2019","citedBy":9,"category":{"type":"JOURNAL","categoryTitle":"IEEE Transactions on Software Engineering","pages":"","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f6b","authors":["Sen Chen","Lingling Fan","Chunyang Chen","Ting Su","Wenhe Li","Yang Liu","Lihua Xu"],"title":"Storydroid: Automated generation of storyboard for Android apps","link":"https://arxiv.org/pdf/1902.00476","description":"Mobile apps are now ubiquitous. Before developing a new app, the development team usually endeavors painstaking efforts to review many existing apps with similar purposes. The review process is crucial in the sense that it reduces market risks and provides inspiration for app development. However, manual exploration of hundreds of existing apps by different roles (e.g., product manager, UI/UX designer, developer) in a development team can be ineffective. For example, it is difficult to completely explore all the functionalities of the app in a short period of time. Inspired by the conception of storyboard in movie production, we propose a system, StoryDroid, to automatically generate the storyboard for Android apps, and assist different roles to review apps efficiently. Specifically, StoryDroid extracts the activity transition graph and leverages static analysis techniques to render UI pages to visualize the storyboard\xa0\u2026","yearPublished":"2019","citedBy":30,"category":{"type":"CONFERENCE","categoryTitle":"2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)","pages":"596-607","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f6e","authors":["Simon Colton","Alison Pease","Michael Cook","Chunyang Chen"],"title":"The HR3 system for automated code generation in creative settings","link":"https://research.monash.edu/files/324969135/324817276_oa.pdf","description":"We describe the HR3 system for automated code generation, and its use in creative tasks. We outline the motivations and overall ideology behind its construction, most notably by identifying some distinctions in AI methodology which can be ignored when AI tasks are viewed as code generation problems to be solved. We further describe the nature of the approach in terms of: a programmatic interface to a Java API; production rule-based batch processing of data; on-demand code generation and inspection, and the usage of randomised and meta-level codebases. To support the claim that the approach is general purpose, we describe five applications in three areas normally covered by separate Computational Creativity systems, namely mathematical discovery, datamining and generative art. We end by discussing future directions for the HR3 system and how this project might address some higher-level issues in Computational Creativity.","yearPublished":"2019","citedBy":2,"category":{"type":"JOURNAL","categoryTitle":"Proceedings of the 10th ICCC","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f6a","authors":["Chunyang Chen","Zhenchang Xing","Yang Liu"],"title":"What\u2019s spain\u2019s paris? mining analogical libraries from q&a discussions","link":"","description":"Third-party libraries are an integral part of many software projects. It often happens that developers need to find analogical libraries that can provide comparable features to the libraries they are already familiar with for different programming languages or different mobile platforms. Existing methods to find analogical libraries are limited by the community-curated list of libraries, blogs, or Q&A posts, which often contain overwhelming or out-of-date information. In this paper, we present a new approach to recommend analogical libraries based on a knowledge base of analogical libraries mined from tags of millions of Stack Overflow questions. The novelty of our approach is to solve analogical-library questions by combining state-of-the-art word embedding technique and domain-specific relational and categorical knowledge mined from Stack Overflow. Given a library and a recommended analogical library, our\xa0\u2026","yearPublished":"2019","citedBy":15,"category":{"type":"JOURNAL","categoryTitle":"Empirical Software Engineering","pages":"1155-1194","publisher":"Springer US","volume":"24","issue":"3"},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2019},{"_id":"610dff68da292ca87c707f75","authors":["Chunyang Chen","Xi Chen","Jiamou Sun","Zhenchang Xing","Guoqiang Li"],"title":"Data-driven proactive policy assurance of post quality in community q&a sites","link":"https://www.researchgate.net/profile/Guoqiang-Li-22/publication/328681390_Data-Driven_Proactive_Policy_Assurance_of_Post_Quality_in_Community_qa_Sites/links/5f7f06e2458515b7cf6f6600/Data-Driven-Proactive-Policy-Assurance-of-Post-Quality-in-Community-q-a-Sites.pdf","description":"To ensure the post quality, Q&A sites usually develop a list of quality assurance guidelines for \\"dos and don\'ts\\", and adopt collaborative editing mechanism to fix quality violations. Quality guidelines are mostly high-level principles, and many tacit and context-sensitive aspects of the expected quality cannot be easily enforced by a set of explicit rules. Collaborative editing is a reactive mechanism after low-quality posts have been posted. Our study of collaborative editing data on Stack Overflow suggests that tacit and context-sensitive quality-assurance knowledge is manifested in the editing patterns of large numbers of collaborative edits. Inspired by this observation, we develop and evaluate a Convolutional Neural Network based approach to learn editing patterns from historical post edits for predicting the need of editing a post. Our approach provides a proactive policy assurance mechanism that warns users\xa0\u2026","yearPublished":"2018","citedBy":21,"category":{"type":"JOURNAL","categoryTitle":"Proceedings of the ACM on Human-Computer Interaction","pages":"1-22","publisher":"ACM","volume":"2","issue":"CSCW"},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2018},{"_id":"610dff68da292ca87c707f72","authors":["Lei Ma","Felix Juefei-Xu","Fuyuan Zhang","Jiyuan Sun","Minhui Xue","Bo Li","Chunyang Chen","Ting Su","Li Li","Yang Liu","Jianjun Zhao","Yadong Wang"],"title":"Deepgauge: Multi-granularity testing criteria for deep learning systems","link":"https://arxiv.org/pdf/1803.07519","description":"Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could\xa0\u2026","yearPublished":"2018","citedBy":297,"category":{"type":"BOOK","categoryTitle":"Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering","pages":"120-131","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2018},{"_id":"610dff68da292ca87c707f74","authors":["Chunyang Chen"],"title":"Distilling crowd knowledge from software-specific Q&A discussions for assisting developers\u2019 knowledge search","link":"https://www.researchgate.net/profile/Chunyang-Chen-2/publication/336889952_Distilling_crowd_knowledge_from_software-specific_QA_discussions_for_assisting_developers\'_knowledge_search/links/5db908eb92851c818014a56f/Distilling-crowd-knowledge-from-software-specific-Q-A-discussions-for-assisting-developers-knowledge-search.pdf","description":"With software penetrating all kinds of traditional or emerging industries, and rapid development of information technology, there is a great demand on software development. In addition, as the task gets more and more complicated, the corresponding software also becomes more and more complex 1. For example, some software are implemented with millions of lines of source code such as Windows 2, Jet control system 3. However, due to the limited number of developers, there is an urgent need to boost developers\u2019 productivity.\\nDuring the evolution of software engineering, a lot of software development experience has been recorded in different software repositories such as code in GitHub, Q&A discussions in Stack Overflow, bug reports in Bugzilla, software vulnerabilities in CVE database, etc. These heterogeneous repositories created by millions of developers serve as the big data of software engineering practices. Reusing the experience in the big data can help avoid re-implementing the wheels, which improves developers\u2019 productivity.","yearPublished":"2018","citedBy":null,"category":{"type":"OTHER","categoryTitle":"","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2018},{"_id":"610dff68da292ca87c707f73","authors":["Chunyang Chen","Ting Su","Guozhu Meng","Zhenchang Xing","Yang Liu"],"title":"From ui design image to gui skeleton: a neural machine translator to bootstrap mobile gui implementation","link":"http://acm.mementodepot.org/pubs/proceedings/acmconferences_3180155/3180155/3180155.3180240/3180155.3180240.pdf","description":"A GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features\' spatial layouts\xa0\u2026","yearPublished":"2018","citedBy":80,"category":{"type":"BOOK","categoryTitle":"Proceedings of the 40th International Conference on Software Engineering","pages":"665-676","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2018},{"_id":"610dff68da292ca87c707f71","authors":["Yi Huang","Chunyang Chen","Zhenchang Xing","Tian Lin","Yang Liu"],"title":"Tell them apart: distilling technology differences from crowd-scale comparison discussions","link":"https://www.researchgate.net/profile/Chunyang-Chen-2/publication/327131742_Tell_them_apart_distilling_technology_differences_from_crowd-scale_comparison_discussions/links/5b9f514e299bf13e6037ce25/Tell-them-apart-distilling-technology-differences-from-crowd-scale-comparison-discussions.pdf","description":"Developers can use different technologies for many software development tasks in their work. However, when faced with several technologies with comparable functionalities, it is not easy for developers to select the most appropriate one, as comparisons among technologies are time-consuming by trial and error. Instead, developers can resort to expert articles, read official documents or ask questions in Q&A sites for technology comparison, but it is opportunistic to get a comprehensive comparison as online information is often fragmented or contradictory. To overcome these limitations, we propose the diffTech system that exploits the crowdsourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We first build a large database of comparable software technologies by mining tags in Stack Overflow, and locate comparative sentences\xa0\u2026","yearPublished":"2018","citedBy":17,"category":{"type":"CONFERENCE","categoryTitle":"2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)","pages":"214-224","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.911Z","updatedAt":"2021-08-07T03:35:04.911Z","year":2018},{"_id":"610dff68da292ca87c707f76","authors":["Chunyang Chen","Zhenchang Xing","Yang Liu"],"title":"By the Community & For the Community: A Deep Learning Approach to Assist Collaborative Editing in Q&A Sites","link":"","description":"Community edits to questions and answers (called post edits) plays an important role in improving content quality in Stack Overflow. Our study of post edits in Stack Overflow shows that a large number of edits are about formatting, grammar and spelling. These post edits usually involve small-scale sentence edits and our survey of trusted contributors suggests that most of them care much or very much about such small sentence edits. To assist users in making small sentence edits, we develop an edit-assistance tool for identifying minor textual issues in posts and recommending sentence edits for correction. We formulate the sentence editing task as a machine translation problem, in which an original sentence is \\"translated\\" into an edited sentence. Our tool implements a character-level Recurrent Neural Network (RNN) encoder-decoder model, trained with about 6.8 millions original-edited sentence pairs from Stack\xa0\u2026","yearPublished":"2017","citedBy":31,"category":{"type":"JOURNAL","categoryTitle":"Proceedings of the ACM on Human-Computer Interaction","pages":"21","publisher":"ACM","volume":"1","issue":"CSCW"},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2017},{"_id":"610dff68da292ca87c707f77","authors":["Chunyang Chen","Zhenchang Xing","Ximing Wang"],"title":"Unsupervised Software-Specific Morphological Forms Inference from Informal Discussions","link":"","description":"Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build\xa0\u2026","yearPublished":"2017","citedBy":47,"category":{"type":"CONFERENCE","categoryTitle":"39th International Conference on Software Engineering (ICSE)","pages":"450-461","publisher":"IEEE/ACM","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2017},{"_id":"610dff68da292ca87c707f79","authors":["Guibin Chen","Chunyang Chen","Zhenchang Xing","Bowen Xu"],"title":"Learning a dual-language vector space for domain-specific cross-lingual question retrieval","link":"","description":"The lingual barrier limits the ability of millions of non-English speaking developers to make effective use of the tremendous knowledge in Stack Overflow, which is archived in English. For cross-lingual question retrieval, one may use translation-based methods that first translate the non-English queries into English and then perform monolingual question retrieval in English. However, translation-based methods suffer from semantic deviation due to inappropriate translation, especially for domain-specific terms, and lexical gap between queries and questions that share few words in common. To overcome the above issues, we propose a novel cross-lingual question retrieval based on word embed-dings and convolutional neural network (CNN) which are the state-of-the-art deep learning techniques to capture word- and sentence-level semantics. The CNN model is trained with large amounts of examples from Stack\xa0\u2026","yearPublished":"2016","citedBy":46,"category":{"type":"CONFERENCE","categoryTitle":"2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE)","pages":"744-755","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2016},{"_id":"610dff68da292ca87c707f7c","authors":["Chunyang Chen","Sa Gao","Zhenchang Xing"],"title":"Mining Analogical Libraries in Q&A Discussions -Incorporating Relational and Categorical Knowledge into Word Embedding","link":"","description":"Third-party libraries are an integral part of many software projects. It often happens that developers need to find analogical libraries that can provide comparable features to the libraries they are already familiar with. Existing methods to find analogical libraries are limited by the community-curated list of libraries, blogs, or Q&A posts, which often contain overwhelming or out-of-date information. In this paper, we present a new approach to recommend analogical libraries based on a knowledge base of analogical libraries mined from tags of millions of Stack Overflow questions. The novelty of our approach is to solve analogical-libraries questions by combining state-of-the-art word embedding technique and domain-specific relational and categorical knowledge mined from Stack Overflow. We implement our approach in a proof-of-concept web application (https://graphofknowledge.appspot.com/similartech). The\xa0\u2026","yearPublished":"2016","citedBy":59,"category":{"type":"CONFERENCE","categoryTitle":"The 23rd IEEE International Conference on Software Analysis, Evolution, and Reengineering (SANER)","pages":"338--348","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.913Z","updatedAt":"2021-08-07T03:35:04.913Z","year":2016},{"_id":"610dff68da292ca87c707f78","authors":["Chunyang Chen","Zhenchang Xing"],"title":"Mining Technology Landscape from Stack Overflow","link":"","description":"The sheer number of available technologies and the complex relationships among them make it challenging to choose the right technologies for software projects. Developers often turn to online resources (eg, expert articles and community answers) to get a good understanding of the technology landscape. Such online resources are primarily opinion-based and are often out of date. Furthermore, information is often scattered in many online resources, which has to be aggregated to have a big picture of the technology landscape. In this paper, we exploit the fact that Stack Overflow users tag their questions with the main technologies that the questions revolve around, and develop association rule mining and community detection techniques to mine technology landscape from Stack Overflow question tags. The mined technology landscape is represented in a graphical Technology Associative Network (TAN). Our\xa0\u2026","yearPublished":"2016","citedBy":48,"category":{"type":"CONFERENCE","categoryTitle":"Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement","pages":"14:1-14:10","publisher":"ACM","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2016},{"_id":"610dff68da292ca87c707f7a","authors":["Chunyang Chen","Zhenchang Xing"],"title":"Similartech: automatically recommend analogical libraries across different programming languages","link":"","description":"Third-party libraries are an integral part of many software projects. It often happens that developers need to find analogical libraries that can provide comparable features to the libraries they are already familiar with. Existing methods to find analogical libraries are limited by the community-curated list of libraries, blogs, or Q&A posts, which often contain overwhelming or out-of-date information. This paper presents our tool SimilarTech (https://graphofknowledge. appspot. com/similartech) that makes it possible to automatically recommend analogical libraries by incorporating tag embeddings and domain-specific relational and categorical knowledge mined from Stack Overflow. SimilarTech currently supports recommendation of 6,715 libraries across 6 different programming languages. We release our SimilarTech website for public use. The SimilarTech website attracts more than 2,400 users in the past 6 months. We\xa0\u2026","yearPublished":"2016","citedBy":32,"category":{"type":"BOOK","categoryTitle":"Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering","pages":"834-839","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2016},{"_id":"610dff68da292ca87c707f7d","authors":["Chunyang Chen","Zhenchang Xing","Lei Han"],"title":"Techland: Assisting technology landscape inquiries with insights from stack overflow","link":"","description":"Understanding the technology landscape is crucial for the success of the software-engineering project or organization. However, it can be difficult, even for experienced developers, due to the proliferation of similar technologies, the complex and often implicit dependencies among technologies, and the rapid development in which technology landscape evolves. Developers currently rely on online documents such as tutorials and blogs to find out best available technologies, technology correlations, and technology trends. Although helpful, online documents often lack objective, consistent summary of the technology landscape. In this paper, we present the TechLand system for assisting technology landscape inquiries with categorical, relational and trending knowledge of technologies that is aggregated from millions of Stack Overflow questions mentioning the relevant technologies. We implement the TechLand\xa0\u2026","yearPublished":"2016","citedBy":28,"category":{"type":"CONFERENCE","categoryTitle":"IEEE International Conference on Software Maintenance and Evolution","pages":"356-366","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.913Z","updatedAt":"2021-08-07T03:35:04.913Z","year":2016},{"_id":"610dff68da292ca87c707f7b","authors":["Chunyang Chen","Zhenchang Xing"],"title":"Towards Correlating Search on Google and Asking on Stack Overflow","link":"https://scholar.archive.org/work/jfgwqwnvk5ea3iat5b4i5hlh2i/access/wayback/http://ccywch.github.io:80/chenchunyang.github.io/publication/ask_search_correlation.pdf","description":"Search engines and Question and Answer (Q&A) sites are the two commonly used ways for developers to seek information on the web. In this paper, we ask whether the questions developers ask on Q&A sites correlate with the information developers search for using search engines. We report on our empirical study to investigate the correlations of the 185 popular technical terms developers search on Google and ask on Stack Overflow using search statistics obtained from Google Trends over a 574-weeks span and question statistics derived from Stack Overflow Data Dump over a 300-weeks span. Our study shows that technical terms searched and asked have strong correlation over time. Search and asking of newer, specific technical terms have stronger correlation, compared with older, general technical terms. We have developed a web interface for accessing our dataset and empirical results available at http\xa0\u2026","yearPublished":"2016","citedBy":26,"category":{"type":"CONFERENCE","categoryTitle":"The 40th IEEE Computer Society International Conference on Computers, Software & Applications","pages":"83-92","publisher":"IEEE","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.912Z","updatedAt":"2021-08-07T03:35:04.912Z","year":2016},{"_id":"610dff68da292ca87c707f7e","authors":["Kelly Blincoe","Zhenchang Xing","Giuliano Antoniol","Francesca Arcelli Fontana","Venera Arnaoudova","Hamid Bagheri","Maria Teresa Baldassarre","Lingfeng Bao","Gabriele Bavota","Olga Baysal","\xc1rp\xe1d Besz\xe9des","Dave Binkley","Fernando Castor","Gemma Catolino","Chunyang Chen","Tse-Hsun Peter Chen","James Clause","Anthony Cleve","Eleni Constantinou","Diego Elias Damasceno Costa","Andrea De Lucia"],"title":"Research Track","link":"https://ieeexplore.ieee.org/iel7/9240597/9240598/09240652.pdf","description":"Page 1. Program Committee Research Track Kelly Blincoe, University of Auckland, New Zealand (Co-Chair) Zhenchang Xing, Australian National University, Australia (Co-Chair) Giuliano Antoniol, Ecole Polytechnique de Montr\xe9al, Canada Francesca Arcelli Fontana, University of Milano - Bicocca, Italy Venera Arnaoudova, Washington State University, USA Hamid Bagheri, University of California, Irvine, USA Maria Teresa Baldassarre, University of Bari, Italy Lingfeng Bao, Zhejiang University, China Gabriele Bavota, Universit\xe0 della Svizzera italiana (USI), Switzerland Olga Baysal, Carleton University, Canada \xc1rp\xe1d Besz\xe9des, University of Szeged, Hungary Dave Binkley, Loyola University Maryland, USA Tegawend\xe9 F. Bissyand\xe9, SnT, University of Luxembourg, Luxembourg Fernando Castor, Universidade Federal de Pernambuco, Brazil Gemma Catolino, Delft University of Technology, The Netherlands\xa0\u2026","yearPublished":"2000","citedBy":null,"category":{"type":"OTHER","categoryTitle":"","pages":"","publisher":"","volume":"","issue":""},"teamId":"60f00ad1f419ce20b8bd930d","__v":0,"createdAt":"2021-08-07T03:35:04.913Z","updatedAt":"2021-08-07T03:35:04.913Z","year":2000}]'),I=JSON.parse('{"_id":"60f00ad1f419ce20b8bd930d","email":"pmyip@gmail.com","teamName":"Group Name ","orgName":"Org Name ","twitterHandle":"PuiYip5"}'),C=JSON.parse('[{"_id":"60f93186bbeeac86fca38c32","fullName":"t3t","position":"123123123","summary":"t3t"},{"_id":"60fb4d185ff50499387c7c04","fullName":"dwed","position":"ewdew","summary":"dewdw"},{"_id":"60fb5af8b8d4afa9a875b859","fullName":"tert","position":"reter","summary":"tretret"}]'),A=function(){var e=w;return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)("div",{className:"text-center mt-3 mb-3",children:Object(h.jsxs)("h4",{children:["Total of ",e.length," publications"]})}),Object(h.jsx)("div",{className:"publicationList",children:e.map((function(e){return Object(h.jsx)(v,{pub:e},e._id)}))})]})},E=function(){var e=I.twitterHandle;return Object(h.jsx)(h.Fragment,{children:Object(h.jsx)(r.a,{fluid:!0,children:Object(h.jsxs)(d.a,{children:[Object(h.jsx)(l.a,{xs:12,md:10,children:Object(h.jsx)(A,{})}),Object(h.jsx)(l.a,{xs:4,md:2,children:e?Object(h.jsx)(g,{linkedHandle:e}):null})]})})})},k=function(){var e=C;return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)("h2",{children:" Team Page "}),e.map((function(e,a){return Object(h.jsxs)("div",{children:[Object(h.jsxs)("h4",{children:[" Member ",a+1," "]}),Object(h.jsxs)("li",{children:[" full Name: ",e.fullName," "]}),Object(h.jsxs)("li",{children:[" position: ",e.position," "]}),Object(h.jsxs)("li",{children:[" summary: ",e.position," "]})]},a)}))]})},T=function(){return Object(h.jsxs)("div",{children:[Object(h.jsx)("h2",{children:" Home Page "}),Object(h.jsx)("li",{children:Object(h.jsx)(s.b,{to:"/publication",children:"Publication"})}),Object(h.jsx)("li",{children:Object(h.jsx)(s.b,{to:"/team",children:"Team"})})]})},x=(t(57),t(6)),j=function(e){var a=e.data,t=Object(x.f)();return Object(h.jsx)("div",{className:"Sidebar",children:Object(h.jsx)("ul",{className:"SidebarList",children:a.map((function(e,a){return Object(h.jsx)(s.b,{to:e.link,children:Object(h.jsxs)("li",{className:"row",id:t.pathname===e.link?"active":"",children:[Object(h.jsx)("div",{id:"icon",children:e.icon}),Object(h.jsx)("div",{id:"title",children:e.title})]})},a)}))})})},O=function(){return[{title:"Home",icon:Object(h.jsx)(m.b,{}),link:"/"},{title:"Publications",icon:Object(h.jsx)(m.a,{}),link:"/publication"},{title:"Team Member",icon:Object(h.jsx)(m.d,{}),link:"/team"}]},U=t(61),_=(t(58),function(){var e=I.orgName,a=I.teamName;return Object(h.jsx)(i.Fragment,{children:Object(h.jsx)(U.a,{className:"header",sticky:"top",children:Object(h.jsx)(U.a.Brand,{children:Object(h.jsxs)("div",{className:"header-brand",children:[a," @ ",e]})})})})}),S=function(){return Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)(_,{}),Object(h.jsx)(r.a,{fluid:!0,children:Object(h.jsxs)(d.a,{children:[Object(h.jsx)(l.a,{className:"sidebar-wrapper",sm:4,md:2,children:Object(h.jsx)(j,{data:O()})}),Object(h.jsx)(l.a,{className:"page-content-wrapper",sm:8,md:10,children:Object(h.jsxs)(x.c,{children:[Object(h.jsx)(x.a,{exact:!0,path:"/",component:T}),Object(h.jsx)(x.a,{exact:!0,path:"/publication",component:E}),Object(h.jsx)(x.a,{exact:!0,path:"/team",component:k})]})})]})})]})};o.a.render(Object(h.jsx)(s.a,{children:Object(h.jsx)(S,{})}),document.getElementById("root"))}},[[59,1,2]]]);
//# sourceMappingURL=main.3f67590d.chunk.js.map